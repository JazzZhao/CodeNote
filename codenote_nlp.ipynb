{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-22T05:51:55.277179Z",
     "iopub.status.busy": "2022-03-22T05:51:55.276837Z",
     "iopub.status.idle": "2022-03-22T05:51:55.296837Z",
     "shell.execute_reply": "2022-03-22T05:51:55.295664Z",
     "shell.execute_reply.started": "2022-03-22T05:51:55.277143Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import utils\n",
    "import re\n",
    "import collections\n",
    "import random\n",
    "# sys.path.append('/kaggle/input')\n",
    "# from myutils import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_time_machines():\n",
    "    with open(\"timemachine.txt\") as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lines, token='word'):\n",
    "    if token == 'word':\n",
    "        return  [ line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('错误，未知词元类型'+token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"文本词表\"\"\"\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        ##按出现频率排序\n",
    "        counter = count_corpus(tokens)\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x : x[1], reverse=True)\n",
    "        #未知词的索引为0\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {token:idx for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq<min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "    \n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "    \n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return 0\n",
    "    \n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self._token_freqs\n",
    "\n",
    "def count_corpus(tokens):  #@save\n",
    "    \"\"\"统计词元的频率\"\"\"\n",
    "    # 这里的tokens是1D列表或2D列表\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # 将词元列表展平成一个列表\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36019, 4942)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_corpus_time_machine(max_tokens=-1):  #@save\n",
    "    \"\"\"返回时光机器数据集的词元索引列表和词表\"\"\"\n",
    "    lines = read_time_machines()\n",
    "    tokens = tokenize(lines, 'word')\n",
    "    vocab = Vocab(tokens)\n",
    "    # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，\n",
    "    # 所以将所有文本行展平到一个列表中\n",
    "    corpus = [vocab[token] for line in tokens for token in line]\n",
    "    if max_tokens > 0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    return corpus, vocab\n",
    "\n",
    "corpus, vocab = load_corpus_time_machine()\n",
    "len(corpus), len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#随机采样\n",
    "def seq_data_iter_random(corpus,batch_size, num_steps):\n",
    "    #随机生成一步内的随机数,并截取corpus\n",
    "    corpus = corpus[random.randint(0, num_steps-1):]\n",
    "    #减去1，是因为我们需要考虑标签\n",
    "    num_subseqs = (len(corpus)-1)//num_steps\n",
    "    initial_indices = list(i for i in range(0, num_subseqs*num_steps, num_steps))\n",
    "    #打乱排序，形成随机抽样\n",
    "    random.shuffle(initial_indices)\n",
    "\n",
    "    def data(pos):\n",
    "        return corpus[pos:pos+num_steps]\n",
    "\n",
    "    num_batches = num_subseqs//batch_size\n",
    "    for i in range(0, num_batches*batch_size, batch_size):\n",
    "        # 在这里，initial_indices包含子序列的随机起始索引\n",
    "        initial_indices_per_batch = initial_indices[i: i+batch_size]\n",
    "        X = [data(j) for j in initial_indices_per_batch]\n",
    "        Y = [data(j+1) for j in initial_indices_per_batch]\n",
    "        yield torch.tensor(X), torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#顺序采样\n",
    "def seq_data_iter_sequential(corpus, batch_size, num_steps):\n",
    "    #生成一个随机数\n",
    "    offset = random.randint(0, num_steps)\n",
    "    #减去1为了减去一个标签\n",
    "    num_tokens = ((len(corpus)-1-offset)//batch_size)*batch_size\n",
    "    Xs = torch.tensor(corpus[offset:offset+num_tokens])\n",
    "    Ys = torch.tensor(corpus[offset+1:offset+num_tokens+1])\n",
    "    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n",
    "    num_batches = Xs.shape[1] // num_steps\n",
    "    for i in range(0, num_batches*num_steps, num_steps):\n",
    "        X = Xs[:,i:i+num_steps]\n",
    "        Y = Ys[:,i:i+num_steps]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[ 2,  3,  4,  5,  6],\n",
      "        [18, 19, 20, 21, 22]]) \n",
      "Y: tensor([[ 3,  4,  5,  6,  7],\n",
      "        [19, 20, 21, 22, 23]])\n",
      "x: tensor([[ 7,  8,  9, 10, 11],\n",
      "        [23, 24, 25, 26, 27]]) \n",
      "Y: tensor([[ 8,  9, 10, 11, 12],\n",
      "        [24, 25, 26, 27, 28]])\n",
      "x: tensor([[12, 13, 14, 15, 16],\n",
      "        [28, 29, 30, 31, 32]]) \n",
      "Y: tensor([[13, 14, 15, 16, 17],\n",
      "        [29, 30, 31, 32, 33]])\n"
     ]
    }
   ],
   "source": [
    "my_seq = list(range(35))\n",
    "for X, Y in seq_data_iter_sequential(my_seq, 2, 5):\n",
    "    print(\"x:\", X, \"\\nY:\", Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.reshape(2,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  5],\n",
       "        [ 6,  7,  8,  9, 10]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75c4182fdda04e01df3e97b8f170bae7db840ddc945de1ed7e60ce57cc68d8b8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
