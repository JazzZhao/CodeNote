{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zjt78\\miniconda3\\envs\\paddle\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\zjt78\\miniconda3\\envs\\paddle\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import paddlenlp\n",
    "from paddlenlp.dataaug import WordSubstitute\n",
    "from paddlenlp.data import Tuple, Pad\n",
    "from paddlenlp.datasets import load_dataset\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from model import SimCSE\n",
    "from data import(\n",
    "    read_simcse_text,\n",
    "    read_text_pair,\n",
    "    convert_example,\n",
    "    create_dataloader,\n",
    "    word_repetition\n",
    ")\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#配置超参数\n",
    "is_unsupervised = False\n",
    "batch_size= 64\n",
    "max_steps = -1\n",
    "output_emb_size=256\n",
    "dropout=0.1\n",
    "scale=20\n",
    "margin=0.1\n",
    "epochs= 3\n",
    "learning_rate= 5E-5\n",
    "warmup_proportion = 0.0\n",
    "weight_decay=0.0\n",
    "dup_rate=0.3\n",
    "save_dir='checkpoints'\n",
    "save_steps=10\n",
    "max_seq_length=64\n",
    "device=\"cpu\"\n",
    "train_set_file=\"baoxian/train_aug.csv\"\n",
    "model_name_or_path = \"rocketqa-zh-base-query-encoder\"\n",
    "seed = 1000\n",
    "rdrop_coef = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"sets random seed\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    paddle.seed(seed)\n",
    "    \n",
    "def do_train():\n",
    "    paddle.set_device(device)\n",
    "    rank = paddle.distributed.get_rank()\n",
    "    if paddle.distributed.get_world_size() > 1:\n",
    "        paddle.distributed.init_parallel_env()\n",
    "\n",
    "    set_seed(seed)\n",
    "    if is_unsupervised:\n",
    "        train_ds = load_dataset(read_simcse_text, data_path=train_set_file, is_test=False, lazy=False)\n",
    "    else:\n",
    "        train_ds = load_dataset(read_text_pair, data_path=train_set_file, is_test=False, lazy=False)\n",
    "\n",
    "    pretrained_model = paddlenlp.transformers.ErnieModel.from_pretrained(model_name_or_path)\n",
    "\n",
    "    tokenizer = paddlenlp.transformers.ErnieTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "    trans_func = partial(\n",
    "        convert_example,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=max_seq_length)\n",
    "\n",
    "    def batchify_fn(\n",
    "        samples,\n",
    "        fn=Tuple(\n",
    "            Pad(axis=0, pad_val=tokenizer.pad_token_id, dtype=\"int64\"),  # query_input\n",
    "            Pad(axis=0, pad_val=tokenizer.pad_token_type_id, dtype=\"int64\"),  # query_segment\n",
    "            Pad(axis=0, pad_val=tokenizer.pad_token_id, dtype=\"int64\"),  # title_input\n",
    "            Pad(axis=0, pad_val=tokenizer.pad_token_type_id, dtype=\"int64\"),  # title_segment\n",
    "        ),\n",
    "    ):\n",
    "        return [data for data in fn(samples)]\n",
    "\n",
    "    train_data_loader = create_dataloader(\n",
    "        train_ds,\n",
    "        mode='train',\n",
    "        batch_size=batch_size,\n",
    "        batchify_fn=batchify_fn,\n",
    "        trans_fn=trans_func)\n",
    "\n",
    "    model = SimCSE(\n",
    "        pretrained_model,\n",
    "        margin=margin,\n",
    "        scale=scale,\n",
    "        output_emb_size=output_emb_size)\n",
    "\n",
    "    # if init_from_ckpt and os.path.isfile(init_from_ckpt):\n",
    "    #     state_dict = paddle.load(args.init_from_ckpt)\n",
    "    #     model.set_dict(state_dict)\n",
    "    #     print(\"warmup from:{}\".format(args.init_from_ckpt))\n",
    "\n",
    "    model = paddle.DataParallel(model)\n",
    "\n",
    "    num_training_steps = max_steps if max_steps > 0 else len(\n",
    "        train_data_loader) * epochs\n",
    "\n",
    "    lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps,\n",
    "                                         warmup_proportion)\n",
    "\n",
    "    # Generate parameter names needed to perform weight decay.\n",
    "    # All bias and LayerNorm parameters are excluded.\n",
    "    decay_params = [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ]\n",
    "    optimizer = paddle.optimizer.AdamW(\n",
    "        learning_rate=lr_scheduler,\n",
    "        parameters=model.parameters(),\n",
    "        weight_decay=weight_decay,\n",
    "        apply_decay_param_fun=lambda x: x in decay_params)\n",
    "\n",
    "    global_step = 0\n",
    "    tic_train = time.time()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for step, batch in enumerate(train_data_loader, start=1):\n",
    "            query_input_ids, query_token_type_ids, title_input_ids, title_token_type_ids = batch\n",
    "            if random.random() < 0.2:\n",
    "                title_input_ids, title_token_type_ids = query_input_ids, query_token_type_ids\n",
    "                query_input_ids, query_token_type_ids = word_repetition(query_input_ids, query_token_type_ids, dup_rate)\n",
    "                title_input_ids, title_token_type_ids = word_repetition(title_input_ids, title_token_type_ids, dup_rate)\n",
    "\n",
    "            loss, kl_loss = model(\n",
    "                query_input_ids=query_input_ids,\n",
    "                title_input_ids=title_input_ids,\n",
    "                query_token_type_ids=query_token_type_ids,\n",
    "                title_token_type_ids=title_token_type_ids)\n",
    "\n",
    "            loss = loss + kl_loss * rdrop_coef\n",
    "\n",
    "            global_step += 1\n",
    "            if global_step % 10 == 0 and rank == 0:\n",
    "                print(\n",
    "                    \"global step %d, epoch: %d, batch: %d, loss: %.5f, speed: %.2f step/s\"\n",
    "                    % (global_step, epoch, step, loss,\n",
    "                       10 / (time.time() - tic_train)))\n",
    "                tic_train = time.time()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.clear_grad()\n",
    "            if global_step % save_steps == 0 and rank == 0:\n",
    "                save_dir = os.path.join(save_dir, \"model_%d\" % global_step)\n",
    "                if not os.path.exists(save_dir):\n",
    "                    os.makedirs(save_dir)\n",
    "                save_param_path = os.path.join(save_dir, 'model_state.pdparams')\n",
    "                paddle.save(model.state_dict(), save_param_path)\n",
    "                tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "            if max_steps > 0 and global_step >= max_steps:\n",
    "                return\n",
    "\n",
    "    save_dir = os.path.join(args.save_dir, \"model_%d\" % global_step)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        save_param_path = os.path.join(save_dir, 'model_state.pdparams')\n",
    "        paddle.save(model.state_dict(), save_param_path)\n",
    "        tokenizer.save_pretrained(save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a48dfe28e322957f704fed4a234742707446a0393501f475af07d20091f93ee6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
